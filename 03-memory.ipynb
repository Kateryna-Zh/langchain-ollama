{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "132d6dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model_name = \"llama3.2\"\n",
    "llm = ChatOllama(model=model_name, temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e62fa",
   "metadata": {},
   "source": [
    "ConversationBufferMemory with RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5085f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "system_message = \"You are a helpful assistant called AlphaBot.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_message),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ea0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7db790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "\n",
    "def get_chat_history(user_id: str) -> InMemoryChatMessageHistory:\n",
    "    if user_id not in chat_map:\n",
    "        chat_map[user_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[user_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f294304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    runnable=pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"user_input\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feb4621b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Kateryna! It's lovely to meet you. I'm AlphaBot, your friendly AI assistant. How can I help you today? Do you have any questions or topics you'd like to discuss? I'm all ears (or rather, all text)!\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-12-04T15:21:37.756666Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3710974875, 'load_duration': 2610546292, 'prompt_eval_count': 43, 'prompt_eval_duration': 246010208, 'eval_count': 54, 'eval_duration': 556659374, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--5e8861c1-3b6c-4cc6-94cf-81d04d40ea0f-0', usage_metadata={'input_tokens': 43, 'output_tokens': 54, 'total_tokens': 97})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"user_input\": \"Hello, my name is Kateryna!\"},\n",
    "    config={\"session_id\": \"user_1\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54c4ee69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Your name is Kateryna! Don't worry if you forgot - I've got your back. What else can I help you with today?\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-12-04T15:23:01.911295Z', 'done': True, 'done_reason': 'stop', 'total_duration': 664331500, 'load_duration': 99163583, 'prompt_eval_count': 115, 'prompt_eval_duration': 110378375, 'eval_count': 30, 'eval_duration': 309687583, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--b0945706-2646-4eb5-9aee-0eea87866c27-0', usage_metadata={'input_tokens': 115, 'output_tokens': 30, 'total_tokens': 145})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"user_input\": \"Can you remind me what my name is?\"},\n",
    "    config={\"session_id\": \"user_1\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3d6626",
   "metadata": {},
   "source": [
    "ConversationBufferWindowMemory with RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4391dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"Chat message history that stores messages in a buffer with a fixed window size.\"\"\"\n",
    "\n",
    "    window_size: int = Field(default_factory=int, description=\"The maximum number of messages to store in the buffer.\")\n",
    "    messages: list[BaseMessage] = Field(default_factory=list, description=\"The buffer to store messages.\")\n",
    "\n",
    "    def __init__(self, window_size: int = 5) -> None:\n",
    "        super().__init__(window_size=window_size)\n",
    "        print(f\"Initialized BufferWindowMessageHistory with window size: {window_size}\" )\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add a messages to the buffer, maintaining the window size.\"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-self.window_size:]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8529da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "\n",
    "def get_chat_history(session_id: str, k: int = 5) -> BufferWindowMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = BufferWindowMessageHistory(window_size=k)\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c4be141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"user_input\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history.\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"Window Size\",\n",
    "            description=\"The number of messages to retain in the chat history.\",\n",
    "            default=4,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d165cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized BufferWindowMessageHistory with window size: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Kate! It's nice to meet you. I'm AlphaBot, your friendly AI assistant. How can I help you today? Do you have any questions or topics you'd like to discuss? I'm all ears (or rather, all text).\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-12-04T16:48:53.612654Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3640917208, 'load_duration': 2605008875, 'prompt_eval_count': 40, 'prompt_eval_duration': 228314375, 'eval_count': 52, 'eval_duration': 538555499, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--b248616d-d7d3-4bc9-89ff-5ff05a852559-0', usage_metadata={'input_tokens': 40, 'output_tokens': 52, 'total_tokens': 92})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"user_input\": \"Hi, my name is Kate\"},\n",
    "    config={\"configurable\" : { \"session_id\": \"id_k4\", \"k\": 4 }}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d02262ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k4\"].clear()  # clear the history\n",
    "\n",
    "# manually insert history\n",
    "chat_map[\"id_k4\"].add_user_message(\"Hi, my name is Josh\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"I'm an AI model called Zeta.\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"id_k4\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99310e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You didn\\'t tell me your name. You introduced yourself as \"you\" when you started our conversation. I\\'m AlphaBot, your helpful assistant.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-12-04T16:52:12.090188Z', 'done': True, 'done_reason': 'stop', 'total_duration': 677279084, 'load_duration': 100080875, 'prompt_eval_count': 97, 'prompt_eval_duration': 93829709, 'eval_count': 31, 'eval_duration': 318803539, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--bbb51aed-eb4f-42f7-a3be-d9e32496af64-0', usage_metadata={'input_tokens': 97, 'output_tokens': 31, 'total_tokens': 128})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#when running with k=4 we should expect the LLM to forget name:\n",
    "pipeline_with_history.invoke(\n",
    "    {\"user_input\": \"What is my name again?\"},\n",
    "    config={\"configurable\" : { \"session_id\": \"id_k4\", \"k\": 4 }}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72509edd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized BufferWindowMessageHistory with window size: 14\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Kate! It's nice to meet you. I'm AlphaBot, your friendly AI assistant. How can I help you today? Do you have any questions or topics you'd like to discuss? I'm all ears (or rather, all text).\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-12-04T16:54:43.155109Z', 'done': True, 'done_reason': 'stop', 'total_duration': 988566292, 'load_duration': 98924750, 'prompt_eval_count': 40, 'prompt_eval_duration': 95991125, 'eval_count': 52, 'eval_duration': 536806164, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--5553ad97-e8e6-4664-b336-dd7941ca2201-0', usage_metadata={'input_tokens': 40, 'output_tokens': 52, 'total_tokens': 92})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#increasing k should help the LLM remember the name:\n",
    "pipeline_with_history.invoke(\n",
    "    {\"user_input\": \"Hi, my name is Kate\"},\n",
    "    config={\"configurable\" : { \"session_id\": \"id_k14\", \"k\": 14 }}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "07903989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is Kate', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hello Kate! It's nice to meet you. I'm AlphaBot, your friendly AI assistant. How can I help you today? Do you have any questions or topics you'd like to discuss? I'm all ears (or rather, all text).\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-12-04T16:54:43.155109Z', 'done': True, 'done_reason': 'stop', 'total_duration': 988566292, 'load_duration': 98924750, 'prompt_eval_count': 40, 'prompt_eval_duration': 95991125, 'eval_count': 52, 'eval_duration': 536806164, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--5553ad97-e8e6-4664-b336-dd7941ca2201-0', usage_metadata={'input_tokens': 40, 'output_tokens': 52, 'total_tokens': 92}),\n",
       " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k14\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k14\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k14\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"id_k14\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "931546da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Kate.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-12-04T16:55:30.581605Z', 'done': True, 'done_reason': 'stop', 'total_duration': 402577000, 'load_duration': 100957666, 'prompt_eval_count': 227, 'prompt_eval_duration': 201076250, 'eval_count': 6, 'eval_duration': 57043875, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--3435e1fc-93f3-4260-88e4-498b3df7807b-0', usage_metadata={'input_tokens': 227, 'output_tokens': 6, 'total_tokens': 233})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"user_input\": \"What is my name again?\"},\n",
    "    config={\"configurable\" : { \"session_id\": \"id_k14\", \"k\": 14 }}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4add23",
   "metadata": {},
   "source": [
    "ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "70947b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"Chat message history that stores a summary of the conversation.\"\"\"\n",
    "\n",
    "    messages: list[BaseMessage] = Field(default_factory=list, description=\"The buffer to store messages.\")\n",
    "    llm: ChatOllama\n",
    "    summary: str=\"\"\n",
    "\n",
    "    def __init__(self, llm: ChatOllama):\n",
    "        super().__init__(llm=llm)\n",
    "        \n",
    "    \n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history and update the summary.\"\"\"\n",
    "        new_messages_text = \"\\n\".join([f\"{msg.type}: {msg.content}\" for msg in messages])   \n",
    "        \n",
    "        # Update summary logic\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much relevant information as possible BUT keep the summary \"\n",
    "                \"concise and no more than a short paragraph in length.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"New messages:\\n{messages}\"\n",
    "            )\n",
    "        ])\n",
    "        #format messages and invoke llm\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=self.summary,\n",
    "                messages=new_messages_text\n",
    "            )\n",
    "        ).content\n",
    "\n",
    "        self.summary = new_summary\n",
    "        self.messages = [SystemMessage(content=self.summary)]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear chat history.\"\"\"\n",
    "        self.messages = []\n",
    "        self.summary = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3f11636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, llm: ChatOllama) -> ConversationSummaryMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4f13e389",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"user_input\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history.\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatOllama,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for summarization.\",\n",
    "            default=llm,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e028ef5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Kate! It's nice to meet you. I'm AlphaBot, your friendly AI assistant. How can I help you today? Do you have any questions or topics you'd like to discuss? I'm all ears (or rather, all text).\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-12-08T15:21:52.327127Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1000595625, 'load_duration': 98964666, 'prompt_eval_count': 40, 'prompt_eval_duration': 98963375, 'eval_count': 52, 'eval_duration': 534632088, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--6768b20c-7cb6-4e98-b3c7-5297fdda4407-0', usage_metadata={'input_tokens': 40, 'output_tokens': 52, 'total_tokens': 92})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chat_map[\"id_123\"].clear()\n",
    "pipeline_with_history.invoke(\n",
    "    {\"user_input\": \"Hi, my name is Kate\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e56bf32f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"Here is a new summary of the conversation:\\n\\nKate has initiated a conversation with AlphaBot, her friendly AI assistant. Kate introduced herself and expressed interest in discussing something with AlphaBot, but hasn't specified a particular topic or question yet.\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dab9e662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Here is a new summary of the conversation:\\n\\nKate initiated a conversation with AlphaBot, expressing interest in discussing conversational memory. The human responded by asking about different types of conversational memory, and AlphaBot explained that there are two primary types: short-term and long-term conversational memory. Additionally, researchers have identified subtypes such as working memory, episodic memory, and semantic memory, which often overlap or interact with each other. Kate is now interested in exploring a specific aspect of conversational memory further.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"user_input\": \"I'm researching the different types of conversational memory.\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    ")\n",
    "\n",
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b3d1ab90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for msg in [\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]:\n",
    "    pipeline_with_history.invoke(\n",
    "        {\"user_input\": msg},\n",
    "        config={\"session_id\": \"id_123\", \"llm\": llm}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "67b69ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Here is a new summary of the conversation:\\n\\nKate discussed conversational memory with AlphaBot, exploring concepts such as ConversationBufferMemory and ConversationBufferWindowMemory. The human clarified that Buffer Window Memory stores only the last k messages, dropping older ones. This approach has benefits like reduced storage requirements, improved performance, and simplified management, but also limitations such as lost context and inconsistent user experience. Kate expressed interest in learning more about implementing Buffer Window Memory in chatbots and human-computer interaction systems.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_123\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e87d8440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I don't have any information about your name from our previous conversation. This is the start of our conversation, and I'm happy to chat with you! What would you like to talk about?\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-12-08T15:26:37.713465Z', 'done': True, 'done_reason': 'stop', 'total_duration': 868941250, 'load_duration': 99781375, 'prompt_eval_count': 138, 'prompt_eval_duration': 152945667, 'eval_count': 40, 'eval_duration': 419427083, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--07e7cfd4-f5d2-4f76-90ce-19eaee663c2b-0', usage_metadata={'input_tokens': 138, 'output_tokens': 40, 'total_tokens': 178})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"user_input\": \"What is my name again?\"},\n",
    "    config={\"session_id\": \"id_123\", \"llm\": llm} \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab9e2f",
   "metadata": {},
   "source": [
    "ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c23b098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    messages: list[BaseMessage] = Field(default_factory=list)\n",
    "    llm: ChatOllama = Field(default_factory=ChatOllama)\n",
    "    window_size: int = Field(default_factory=int)\n",
    "\n",
    "    def __init__(self, window_size: int, llm: ChatOllama) -> None:\n",
    "        super().__init__(window_size=window_size, llm=llm)\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add messages to the history, removing any messages beyond\n",
    "        the last `k` messages and summarizing the messages that we\n",
    "        drop.\n",
    "        \"\"\"\n",
    "        existing_summary = \"\"\n",
    "        old_messages = []\n",
    "        #check if we already have a summary message\n",
    "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
    "            print(\">> Found existing summary message.\")\n",
    "            existing_summary = self.messages.pop(0)\n",
    "        self.messages.extend(messages)\n",
    "\n",
    "        #check if we have too many messages\n",
    "        if len(self.messages) > self.window_size:\n",
    "            print(\n",
    "                f\">> Found {len(self.messages)} messages, dropping \"\n",
    "                f\"latest {len(self.messages) - self.window_size} messages.\")\n",
    "            # pull out the oldest messages...\n",
    "            old_messages = self.messages[:self.window_size]\n",
    "            # ...and keep only the most recent messages\n",
    "            self.messages = self.messages[-self.window_size:]\n",
    "        if not old_messages:\n",
    "            print(\">> No old messages to update summary with\")\n",
    "            # if we have no old_messages, we have nothing to update in summary\n",
    "            return\n",
    "        # Update summary logic\n",
    "        summary_prompt = ChatPromptTemplate.from_messages([\n",
    "            SystemMessagePromptTemplate.from_template(\n",
    "                \"Given the existing conversation summary and the new messages, \"\n",
    "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
    "                \"as much relevant information as possible BUT keep the summary \"\n",
    "                \"concise and no more than a short paragraph in length.\"\n",
    "            ),\n",
    "            HumanMessagePromptTemplate.from_template(\n",
    "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
    "                \"New messages:\\n{old_messages}\"\n",
    "            )\n",
    "        ])\n",
    "        #format messages and invoke llm\n",
    "        new_summary = self.llm.invoke(\n",
    "            summary_prompt.format_messages(\n",
    "                existing_summary=existing_summary,\n",
    "                old_messages=old_messages\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(f\">> New summary: {new_summary.content}\")\n",
    "        # prepend the new summary to the history\n",
    "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear the history.\"\"\"\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "970386cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "def get_chat_history(session_id: str, window_size: int, llm: ChatOllama) -> ConversationSummaryBufferMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = ConversationSummaryBufferMessageHistory(window_size=window_size, llm=llm)\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c2e41185",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"user_input\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history.\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"window_size\",\n",
    "            annotation=int,\n",
    "            name=\"Window Size\",\n",
    "            description=\"The number of messages to retain in the chat history.\",\n",
    "            default=4,\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"llm\",\n",
    "            annotation=ChatOllama,\n",
    "            name=\"LLM\",\n",
    "            description=\"The LLM to use for summarization.\",\n",
    "            default=llm,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6df474a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> No old messages to update summary with\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, my name is Kate', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Hello Kate! It's nice to meet you. I'm AlphaBot, your friendly AI assistant. How can I help you today? Do you have any questions or topics you'd like to discuss? I'm all ears (or rather, all text).\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-12-08T16:23:00.163282Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2903003792, 'load_duration': 1843678917, 'prompt_eval_count': 40, 'prompt_eval_duration': 260694542, 'eval_count': 52, 'eval_duration': 537588580, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--b9550d46-629f-419c-8cee-36db1df3c7cb-0', usage_metadata={'input_tokens': 40, 'output_tokens': 52, 'total_tokens': 92})]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"user_input\": \"Hi, my name is Kate\"},\n",
    "    config={\"session_id\": \"id_123\", \"window_size\": 4, \"llm\": llm} \n",
    ")   \n",
    "chat_map[\"id_123\"].messages    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "dad69f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Message 1\n",
      "---\n",
      "\n",
      ">> No old messages to update summary with\n",
      "---\n",
      "Message 2\n",
      "---\n",
      "\n",
      ">> Found 6 messages, dropping latest 2 messages.\n",
      ">> New summary: Here is a concise summary of the conversation:\n",
      "\n",
      "Kate initiated a conversation with AlphaBot, her AI assistant. She expressed interest in researching conversational memory and received an introduction to various types of conversational memory, including short-term, working, long-term, and contextual memory. The conversation also touched on cognitive load-based memory types, such as low-cognitive-load and high-cognitive-load memory. AlphaBot offered to help Kate explore her interests further, but the conversation was stopped due to a timeout.\n",
      "---\n",
      "Message 3\n",
      "---\n",
      "\n",
      ">> Found existing summary message.\n",
      ">> Found 6 messages, dropping latest 2 messages.\n",
      ">> New summary: Here is a concise summary of the conversation:\n",
      "\n",
      "Kate initiated a conversation with AlphaBot about conversational memory. AlphaBot introduced various types of conversational memory, including short-term, working, long-term, and contextual memory, as well as cognitive load-based memory types like low-cognitive-load and high-cognitive-load memory. The conversation also touched on conversational memory models such as ConversationBufferMemory and ConversationBufferWindowMemory, which involve storing information about previous turns or interactions in a buffer. AlphaBot offered to help Kate explore her interests further, but the conversation was stopped due to a timeout.\n",
      "---\n",
      "Message 4\n",
      "---\n",
      "\n",
      ">> Found existing summary message.\n",
      ">> Found 6 messages, dropping latest 2 messages.\n",
      ">> New summary: Here is a concise summary of the conversation:\n",
      "\n",
      "Kate initiated a conversation with AlphaBot about conversational memory. AlphaBot explained various types of conversational memory, including short-term and long-term memory, as well as cognitive load-based memory types like low-cognitive-load and high-cognitive-load memory. The conversation then focused on two specific models: ConversationBufferMemory and ConversationBufferWindowMemory. Kate was provided with an overview of both models, their advantages and disadvantages, and potential scenarios where one might be more suitable than the other.\n"
     ]
    }
   ],
   "source": [
    "for i, msg in enumerate([\n",
    "    \"I'm researching the different types of conversational memory.\",\n",
    "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
    "    \"Buffer memory just stores the entire conversation\",\n",
    "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
    "]):\n",
    "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
    "    pipeline_with_history.invoke(\n",
    "        {\"user_input\": msg},\n",
    "        config={\"session_id\": \"id_123\", \"llm\": llm, \"window_size\": 4}\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "132d6dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model_name = \"llama3.2\"\n",
    "llm = ChatOllama(model=model_name, temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55e62fa",
   "metadata": {},
   "source": [
    "ConversationBufferMemory with RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5085f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "system_message = \"You are a helpful assistant called AlphaBot.\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_message),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{user_input}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ea0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7db790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chat_map = {}\n",
    "\n",
    "def get_chat_history(user_id: str) -> InMemoryChatMessageHistory:\n",
    "    if user_id not in chat_map:\n",
    "        chat_map[user_id] = InMemoryChatMessageHistory()\n",
    "    return chat_map[user_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f294304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    runnable=pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"user_input\",\n",
    "    history_messages_key=\"history\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feb4621b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Kateryna! It's lovely to meet you. I'm AlphaBot, your friendly AI assistant. How can I help you today? Do you have any questions or topics you'd like to discuss? I'm all ears (or rather, all text)!\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-12-04T15:21:37.756666Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3710974875, 'load_duration': 2610546292, 'prompt_eval_count': 43, 'prompt_eval_duration': 246010208, 'eval_count': 54, 'eval_duration': 556659374, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--5e8861c1-3b6c-4cc6-94cf-81d04d40ea0f-0', usage_metadata={'input_tokens': 43, 'output_tokens': 54, 'total_tokens': 97})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"user_input\": \"Hello, my name is Kateryna!\"},\n",
    "    config={\"session_id\": \"user_1\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54c4ee69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Your name is Kateryna! Don't worry if you forgot - I've got your back. What else can I help you with today?\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-12-04T15:23:01.911295Z', 'done': True, 'done_reason': 'stop', 'total_duration': 664331500, 'load_duration': 99163583, 'prompt_eval_count': 115, 'prompt_eval_duration': 110378375, 'eval_count': 30, 'eval_duration': 309687583, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--b0945706-2646-4eb5-9aee-0eea87866c27-0', usage_metadata={'input_tokens': 115, 'output_tokens': 30, 'total_tokens': 145})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"user_input\": \"Can you remind me what my name is?\"},\n",
    "    config={\"session_id\": \"user_1\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3d6626",
   "metadata": {},
   "source": [
    "ConversationBufferWindowMemory with RunnableWithMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4391dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
    "    \"\"\"Chat message history that stores messages in a buffer with a fixed window size.\"\"\"\n",
    "\n",
    "    window_size: int = Field(default_factory=int, description=\"The maximum number of messages to store in the buffer.\")\n",
    "    messages: list[BaseMessage] = Field(default_factory=list, description=\"The buffer to store messages.\")\n",
    "\n",
    "    def __init__(self, window_size: int = 5) -> None:\n",
    "        super().__init__(window_size=window_size)\n",
    "        print(f\"Initialized BufferWindowMessageHistory with window size: {window_size}\" )\n",
    "\n",
    "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
    "        \"\"\"Add a messages to the buffer, maintaining the window size.\"\"\"\n",
    "        self.messages.extend(messages)\n",
    "        self.messages = self.messages[-self.window_size:]\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        self.messages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8529da6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_map = {}\n",
    "\n",
    "def get_chat_history(session_id: str, k: int = 5) -> BufferWindowMessageHistory:\n",
    "    if session_id not in chat_map:\n",
    "        chat_map[session_id] = BufferWindowMessageHistory(window_size=k)\n",
    "    return chat_map[session_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7c4be141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "pipeline_with_history = RunnableWithMessageHistory(\n",
    "    pipeline,\n",
    "    get_session_history=get_chat_history,\n",
    "    input_messages_key=\"user_input\",\n",
    "    history_messages_key=\"history\",\n",
    "    history_factory_config=[\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            name=\"Session ID\",\n",
    "            description=\"The session ID to use for the chat history.\",\n",
    "            default=\"id_default\",\n",
    "        ),\n",
    "        ConfigurableFieldSpec(\n",
    "            id=\"k\",\n",
    "            annotation=int,\n",
    "            name=\"Window Size\",\n",
    "            description=\"The number of messages to retain in the chat history.\",\n",
    "            default=4,\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d165cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized BufferWindowMessageHistory with window size: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Hello Kate! It's nice to meet you. I'm AlphaBot, your friendly AI assistant. How can I help you today? Do you have any questions or topics you'd like to discuss? I'm all ears (or rather, all text).\", additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-12-04T16:48:53.612654Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3640917208, 'load_duration': 2605008875, 'prompt_eval_count': 40, 'prompt_eval_duration': 228314375, 'eval_count': 52, 'eval_duration': 538555499, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--b248616d-d7d3-4bc9-89ff-5ff05a852559-0', usage_metadata={'input_tokens': 40, 'output_tokens': 52, 'total_tokens': 92})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_with_history.invoke(\n",
    "    {\"user_input\": \"Hi, my name is Kate\"},\n",
    "    config={\"configurable\" : { \"session_id\": \"id_k4\", \"k\": 4 }}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d02262ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_map[\"id_k4\"].clear()  # clear the history\n",
    "\n",
    "# manually insert history\n",
    "chat_map[\"id_k4\"].add_user_message(\"Hi, my name is Josh\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"I'm an AI model called Zeta.\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
    "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
    "chat_map[\"id_k4\"].add_ai_message(\"Very cool!\")\n",
    "\n",
    "chat_map[\"id_k4\"].messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99310e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='You didn\\'t tell me your name. You introduced yourself as \"you\" when you started our conversation. I\\'m AlphaBot, your helpful assistant.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-12-04T16:52:12.090188Z', 'done': True, 'done_reason': 'stop', 'total_duration': 677279084, 'load_duration': 100080875, 'prompt_eval_count': 97, 'prompt_eval_duration': 93829709, 'eval_count': 31, 'eval_duration': 318803539, 'logprobs': None, 'model_name': 'llama3.2', 'model_provider': 'ollama'}, id='lc_run--bbb51aed-eb4f-42f7-a3be-d9e32496af64-0', usage_metadata={'input_tokens': 97, 'output_tokens': 31, 'total_tokens': 128})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#when running with k=4 we should expect the LLM to forget name:\n",
    "pipeline_with_history.invoke(\n",
    "    {\"user_input\": \"What is my name again?\"},\n",
    "    config={\"configurable\" : { \"session_id\": \"id_k4\", \"k\": 4 }}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
